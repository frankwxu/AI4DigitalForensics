{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0fc8d3e",
   "metadata": {},
   "source": [
    "## Step 1: Implment linear regression: replace the gradient function with autograd\n",
    "\n",
    "- Recall key steps for training\n",
    "    - Forward model (1) = compute prediction with model\n",
    "    - Forward model (2) = Compute loss\n",
    "    - Backward = compute gradients\n",
    "    - Update weights \n",
    "\n",
    "- replace np array with pytorch tensor \n",
    "- replace gradient function with `loss.backward()`    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "016485d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: w = 25.779, loss = 5939.33496094\n",
      "epoch 2: w = 22.191, loss = 4291.27685547\n",
      "epoch 3: w = 19.141, loss = 3100.55566406\n",
      "epoch 4: w = 16.549, loss = 2240.25878906\n",
      "epoch 5: w = 14.346, loss = 1618.69470215\n",
      "epoch 6: w = 12.473, loss = 1169.61450195\n",
      "epoch 7: w = 10.881, loss = 845.15423584\n",
      "epoch 8: w = 9.528, loss = 610.73156738\n",
      "epoch 9: w = 8.378, loss = 441.36120605\n",
      "epoch 10: w = 7.400, loss = 318.99108887\n",
      "epoch 11: w = 6.569, loss = 230.57872009\n",
      "epoch 12: w = 5.863, loss = 166.70083618\n",
      "epoch 13: w = 5.262, loss = 120.54901886\n",
      "epoch 14: w = 4.752, loss = 87.20434570\n",
      "epoch 15: w = 4.318, loss = 63.11280441\n",
      "epoch 16: w = 3.949, loss = 45.70667267\n",
      "epoch 17: w = 3.636, loss = 33.13073730\n",
      "epoch 18: w = 3.370, loss = 24.04463005\n",
      "epoch 19: w = 3.143, loss = 17.47991371\n",
      "epoch 20: w = 2.951, loss = 12.73690891\n",
      "Prediction after training: f(6) = 17.704\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "def forward(w, x):\n",
    "    return w * x\n",
    "\n",
    "# MSE as the loss function\n",
    "def loss(y, y_pred):\n",
    "    return ((y_pred - y)**2).mean()\n",
    "\n",
    "# don't need this any more as we use autograd\n",
    "# MSE = j = 1/N * (w*x - y)**2\n",
    "# dJ/dw = 2/N (w*x - y)*x\n",
    "\"\"\"\n",
    "def gradient(x, y, y_pred):\n",
    "    return np.mean(2*x*(y_pred - y))\n",
    "\"\"\"\n",
    "\n",
    "# Train function\n",
    "def train(learning_rate, n_iters, w, X, Y):\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    w = torch.tensor(w, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "    for epoch in range(n_iters):\n",
    "        y_pred = forward(w, X)  # Forward pass\n",
    "        l = loss(Y, y_pred)     # Loss\n",
    "        \n",
    "        # Backward pass, compute autograde\n",
    "        l.backward()        \n",
    "\n",
    "        # Update weights\n",
    "        with torch.no_grad():\n",
    "            w.data -= learning_rate * w.grad\n",
    "        \n",
    "        w.grad.zero_()  # Reset gradients\n",
    "        \n",
    "        # Print using .item() for scalars to avoid NumPy conversion\n",
    "        print(f'epoch {epoch+1}: w = {w.item():.3f}, loss = {l.item():.8f}')\n",
    "        \n",
    "    print(f'Prediction after training: f(6) = {forward(w.item(), 6):.3f}')\n",
    "    \n",
    "    \n",
    "# Define the data, make sure to use torch tensor, not np.array\n",
    "X = torch.tensor([1.0, 2.0, 3, 4], dtype=torch.float32)\n",
    "Y = torch.tensor([2.3, 3.4, 6.5, 6.8], dtype=torch.float32)\n",
    "\n",
    "# Configration\n",
    "learning_rate = 0.01\n",
    "n_iters = 20\n",
    "w_init = 30\n",
    "train(learning_rate, n_iters, w_init, X, Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58726271",
   "metadata": {},
   "source": [
    "## Step 2: Implment linear regression: replace the update weights (gradient descent) with an optimizor\n",
    "- replace loss function with built-in loss function `loss = nn.MSELoss()`\n",
    "- Update weights  with `  optimizer.step()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712d00f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: w = -0.212, b = -0.831, loss = 60.44245148\n",
      "epoch 2: w = 0.140, b = -0.709, loss = 42.06572342\n",
      "epoch 3: w = 0.434, b = -0.607, loss = 29.31435013\n",
      "epoch 4: w = 0.678, b = -0.522, loss = 20.46628952\n",
      "epoch 5: w = 0.881, b = -0.450, loss = 14.32666016\n",
      "epoch 6: w = 1.051, b = -0.390, loss = 10.06634808\n",
      "epoch 7: w = 1.192, b = -0.340, loss = 7.11005878\n",
      "epoch 8: w = 1.309, b = -0.298, loss = 5.05860424\n",
      "epoch 9: w = 1.406, b = -0.262, loss = 3.63499498\n",
      "epoch 10: w = 1.487, b = -0.232, loss = 2.64703655\n",
      "epoch 11: w = 1.555, b = -0.207, loss = 1.96136689\n",
      "epoch 12: w = 1.611, b = -0.185, loss = 1.48545051\n",
      "epoch 13: w = 1.658, b = -0.167, loss = 1.15507805\n",
      "epoch 14: w = 1.696, b = -0.152, loss = 0.92569691\n",
      "epoch 15: w = 1.729, b = -0.139, loss = 0.76639163\n",
      "epoch 16: w = 1.755, b = -0.127, loss = 0.65571183\n",
      "epoch 17: w = 1.777, b = -0.118, loss = 0.57877225\n",
      "epoch 18: w = 1.796, b = -0.109, loss = 0.52524626\n",
      "epoch 19: w = 1.811, b = -0.102, loss = 0.48796645\n",
      "epoch 20: w = 1.823, b = -0.095, loss = 0.46196088\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# replace this with a Linear Model\n",
    "\"\"\"\n",
    "def forward(w, x):\n",
    "    return w * x\n",
    "\"\"\"\n",
    "\n",
    "# don't need this any more as we use autograd\n",
    "# MSE as the loss function\n",
    "\"\"\"\n",
    "def loss(y, y_pred):\n",
    "   return ((y_pred - y)**2).mean()\n",
    "\"\"\"\n",
    "\n",
    "# don't need this any more as we use autograd\n",
    "# MSE = j = 1/N * (w*x - y)**2\n",
    "# dJ/dw = 2/N (w*x - y)*x\n",
    "\"\"\"\n",
    "def gradient(x, y, y_pred):\n",
    "    return np.mean(2*x*(y_pred - y))\n",
    "\"\"\"\n",
    "\n",
    "# Train function\n",
    "def train(n_iters, X, Y):\n",
    "    for epoch in range(n_iters):\n",
    "        y_pred = model(X)  # Forward pass\n",
    "        # l = loss(Y, y_pred)     # Loss\n",
    "        l = criterion(y_pred, Y)\n",
    "        \n",
    "        # Backward pass, compute autograde (directioin of change for each parameter)\n",
    "        l.backward()        \n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        optimizer.zero_grad() # Reset gradients\n",
    "        \n",
    "        # Print using .item() for scalars to avoid NumPy conversion\n",
    "        # Print w and b\n",
    "        w = model.weight.item()  # Scalar value of weight\n",
    "        b = model.bias.item()    # Scalar value of bias\n",
    "        print(f'epoch {epoch+1}: w = {w:.3f}, b = {b:.3f}, loss = {l.item():.8f}')\n",
    "    \n",
    "# Define the data, make sure to use torch tensor, not np.array\n",
    "X = torch.tensor([1.0, 2.0, 3, 4], dtype=torch.float32)\n",
    "X = X.reshape(4, 1)\n",
    "Y = torch.tensor([2.3, 3.4, 6.5, 6.8], dtype=torch.float32)\n",
    "Y = Y.reshape(4, 1)\n",
    "\n",
    "n_samples, n_features = X.shape \n",
    "\n",
    "# Linear model f = wx + b\n",
    "input_size = n_features\n",
    "output_size = 1\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "\n",
    "# Loss and optimizer\n",
    "learning_rate = 0.01\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Stochastic Gradient Descent (SGD)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "\n",
    "n_iters = 20\n",
    "\n",
    "train(n_iters,  X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09623107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for x = 6: 10.844\n"
     ]
    }
   ],
   "source": [
    "# Test the model with x = 6\n",
    "# predicted = model(X).detach().numpy()\n",
    "test_input = torch.tensor([[6.0]], dtype=torch.float32)  # Shape: (1, 1)\n",
    "with torch.no_grad():  # Disable gradient tracking for inference\n",
    "    y_pred = model(test_input)\n",
    "print(f'Prediction for x = 6: {y_pred.item():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ad547f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
