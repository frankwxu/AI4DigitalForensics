{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce2d850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.1675, Accuracy: 95.05%\n",
      "Epoch 2, Loss: 0.0710, Accuracy: 97.94%\n",
      "Epoch 3, Loss: 0.0518, Accuracy: 98.42%\n",
      "Epoch 4, Loss: 0.0462, Accuracy: 98.64%\n",
      "Epoch 5, Loss: 0.0384, Accuracy: 98.87%\n",
      "Epoch 6, Loss: 0.0336, Accuracy: 99.00%\n",
      "Epoch 7, Loss: 0.0323, Accuracy: 99.06%\n",
      "Epoch 8, Loss: 0.0260, Accuracy: 99.22%\n",
      "Epoch 9, Loss: 0.0254, Accuracy: 99.19%\n",
      "Epoch 10, Loss: 0.0226, Accuracy: 99.27%\n",
      "\n",
      "Test Accuracy: 99.34%\n",
      "Training complete. Plots saved as 'mnist_training_metrics.png', 'mnist_confusion_matrix.png', and 'mnist_sample_predictions.png'\n"
     ]
    }
   ],
   "source": [
    "import torch  # PyTorch library for tensor operations and deep learning\n",
    "import torch.nn as nn  # Neural network modules and layers\n",
    "import torch.nn.functional as F  # Functional operations like activations\n",
    "import torch.optim as optim  # Optimization algorithms (e.g., Adam)\n",
    "import torchvision  # Datasets, models, and transforms for computer vision\n",
    "import torchvision.transforms as transforms  # Image preprocessing transformations\n",
    "import matplotlib.pyplot as plt  # Plotting library for visualizations\n",
    "import numpy as np  # Numerical computations for array operations\n",
    "from sklearn.metrics import confusion_matrix  # For computing confusion matrix\n",
    "import seaborn as sns  # Visualization library for heatmap plots\n",
    "\n",
    "# Set random seed for reproducibility across runs\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Define data transformations for preprocessing MNIST dataset\n",
    "# - ToTensor: Converts PIL images to PyTorch tensors (HWC to CHW format)\n",
    "# - Normalize: Normalizes pixel values using MNIST dataset mean (0.1307) and std (0.3081)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Load MNIST training dataset\n",
    "# - root: Directory to store the dataset\n",
    "# - train: True for training set\n",
    "# - download: Download dataset if not already present\n",
    "# - transform: Apply the defined transformations\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                      download=True, transform=transform)\n",
    "# Create DataLoader for training set\n",
    "# - batch_size: 64 images per batch\n",
    "# - shuffle: Randomly shuffle data for better training\n",
    "# - num_workers: 2 subprocesses for faster data loading\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "# Load MNIST test dataset\n",
    "# - train: False for test set\n",
    "# - Same transformations as training set for consistency\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                     download=True, transform=transform)\n",
    "# Create DataLoader for test set\n",
    "# - shuffle: False to maintain order during evaluation\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "# Define class labels for MNIST (digits 0-9)\n",
    "classes = tuple(str(i) for i in range(10))\n",
    "\n",
    "# Define CNN architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # First convolutional layer\n",
    "        # - Input: 1 channel (grayscale)\n",
    "        # - Output: 32 channels\n",
    "        # - Kernel: 3x3, padding=1 to maintain spatial dimensions\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5, padding=0)\n",
    "        # Second convolutional layer\n",
    "        # - Input: 32 channels\n",
    "        # - Output: 64 channels\n",
    "        # - Kernel: 3x3, padding=1\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5, padding=0)\n",
    "        # Max pooling layer\n",
    "        # - Kernel: 2x2, stride=2 (reduces spatial dimensions by half)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # Batch normalization for first conv layer (32 channels)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        # Batch normalization for second conv layer (64 channels)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        # First fully connected layer\n",
    "        # - Input: 64*4*4 (after two pooling layers on 28x28 input)\n",
    "        # - Output: 128 units\n",
    "        self.fc1 = nn.Linear(64 * 4 * 4, 128)\n",
    "        # Second fully connected layer\n",
    "        # - Input: 128 units\n",
    "        # - Output: 10 units (one per class)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        # Dropout layer with 50% probability to prevent overfitting\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network\n",
    "        # Conv1 -> BatchNorm -> ReLU -> MaxPool\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        # Conv2 -> BatchNorm -> ReLU -> MaxPool\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        # Flatten output for fully connected layers\n",
    "        # - Input size: 64 channels, 4x4 spatial dimensions\n",
    "        x = x.view(-1, 64 * 4 * 4)\n",
    "        # Fully connected layer 1 -> ReLU\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # Apply dropout during training\n",
    "        x = self.dropout(x)\n",
    "        # Fully connected layer 2 for classification\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model and move to appropriate device (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Net().to(device)\n",
    "\n",
    "# Define loss function\n",
    "# - CrossEntropyLoss: Combines log softmax and negative log likelihood loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizer\n",
    "# - Adam optimizer with learning rate 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10  # Number of epochs to train\n",
    "train_losses = []  # Store loss for each epoch\n",
    "train_accuracies = []  # Store accuracy for each epoch\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0  # Accumulate loss for the epoch\n",
    "    correct = 0  # Track correct predictions\n",
    "    total = 0  # Track total samples\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # Get inputs and labels, move to device\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        # Zero out gradients to prevent accumulation\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass through the model\n",
    "        outputs = model(inputs)\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Backward pass to compute gradients\n",
    "        loss.backward()\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update running loss\n",
    "        running_loss += loss.item()\n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    # Calculate and store epoch metrics\n",
    "    epoch_loss = running_loss / len(trainloader)\n",
    "    epoch_acc = 100 * correct / total\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracies.append(epoch_acc)\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%\")\n",
    "\n",
    "# Evaluate model on test set\n",
    "model.eval()  # Set model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "all_preds = []  # Store predictions for confusion matrix\n",
    "all_labels = []  # Store true labels\n",
    "with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate and print test accuracy\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f\"\\nTest Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "# Plot training metrics (loss and accuracy)\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot training loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Training Accuracy')\n",
    "plt.title('Training Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('mnist_training_metrics.png')  # Save the plot\n",
    "plt.close()\n",
    "\n",
    "# Plot confusion matrix to visualize class-wise performance\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=classes, yticklabels=classes)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.savefig('mnist_confusion_matrix.png')  # Save the plot\n",
    "plt.close()\n",
    "\n",
    "# Function to unnormalize and display MNIST images\n",
    "def imshow(img):\n",
    "    img = img * 0.3081 + 0.1307  # Unnormalize (reverse mean/std normalization)\n",
    "    npimg = img.numpy()\n",
    "    return npimg[0]  # Return single channel for grayscale image\n",
    "\n",
    "# Display sample test images with predictions\n",
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "images, labels = images[:8].to(device), labels[:8]\n",
    "outputs = model(images)\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(8):\n",
    "    plt.subplot(2, 4, i + 1)\n",
    "    plt.imshow(imshow(images[i].cpu()), cmap='gray')  # Display grayscale image\n",
    "    plt.title(f'Pred: {classes[predicted[i]]}\\nTrue: {classes[labels[i]]}')\n",
    "    plt.axis('off')\n",
    "plt.savefig('mnist_sample_predictions.png')  # Save the plot\n",
    "plt.close()\n",
    "\n",
    "# Print completion message\n",
    "print(\"Training complete. Plots saved as 'mnist_training_metrics.png', 'mnist_confusion_matrix.png', and 'mnist_sample_predictions.png'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
